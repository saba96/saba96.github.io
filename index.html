<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Saba Ahmadi</title>

    <meta name="author" content="Saba Ahmadi from Jon Barron's template">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  About Me
                </p>
                <p>
                  Hi! I’m Saba Ahmadi, a researcher interested in advancing multimodal AI, with a special focus on vision and language. Currently, I’m a research assistant at <a href="https://mila.quebec/en">Mila</a>, working under the guidance of Professor <a href="https://www.iro.umontreal.ca/~agrawal/">Aishwarya Agrawal</a>. Together, we explore ways to make AI systems that not only "see" images but also interpret them in human-like ways, enabling machines to understand complex visual details and express that understanding through natural language.
                </p>
                <p>
                  I recently completed my Master’s in Computer Science, specializing in Artificial Intelligence, at <a href="https://diro.umontreal.ca/accueil/">Université de Montréal</a> and <a href="https://mila.quebec/en">Mila</a>, also under Professor Agrawal’s supervision. Before this, I earned my Bachelor’s in Computer Engineering from <a href=" https://english.iut.ac.ir/"> Isfahan University of Technology</a>, where I was supervised by Professor Elham Mahmoudzadeh.
                </p> 
                <p> 
                  My work primarily focuses on advancing vision-language models, exploring areas like fine-grained visio-linguistic understanding, minimal image editing for model refinement, and efficient adaptation to new tasks with limited data. I’m also interested in robust, automatic evaluation techniques, crucial for building trustworthy AI systems. Every project I take on is driven by the goal of making AI systems more perceptive, adaptable, and expressive—helping bridge the gap between machine perception and human understanding.
                </p>
                <p style="text-align:center">
                  <a href="mailto:saba.ahmadi@mila.quebec">Email</a> &nbsp;/&nbsp;
                  <a href="data/SabaAhmadi-bio.txt">Bio</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.ca/citations?user=zhGGAZgAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/Saba_A96">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/saba-ahmadi/">LinkedIn</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/SabaAhmadi.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/SabaAhmadi.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My key research interests in the vision-language domain include: fine-grained visio-linguistic reasoning, generative image models, interleaved image-text generation models, robust automatic evaluation methods.
                </p>
              </td>
            </tr>
          </tbody>
        </table>
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <!-- Publication 1 -->
      <tr onmouseout="ever_stop()" onmouseover="ever_start()">
        <td style="padding:10px;width:25%;vertical-align:top">
          <div class="one">
            <div class="two" id='ever_image1'>
              <img src='images/visMin.png' width=100%>
            </div>
            <img src='images/visMin.png' width=100%>
          </div>
          <script type="text/javascript">
            function ever_start() {
              document.getElementById('ever_image1').style.opacity = "1";
            }
            function ever_stop() {
              document.getElementById('ever_image1').style.opacity = "0";
            }
            ever_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:top">
            <span class="papertitle">VisMin: Visual Minimal-Change Understanding</span>
          <br>
          <a href="https://rabiul.me/">Rabiul Awal*</a>, 
          <strong>Saba Ahmadi*</strong>, 
          <a href="https://lezhang7.github.io/">Le Zhang*</a>, 
          <a href="https://www.iro.umontreal.ca/~agrawal/">Aishwarya Agrawal</a>
          <br>
          * equal contribution
          <br>
          <em>Neural Information Processing Systems (NeurIPS), 2024</em>
          <br>
          <a href="https://arxiv.org/abs/2407.16772">ArXiv</a>
        </td>
      </tr>

      <!-- Publication 2 -->
      <tr onmouseout="ever_stop()" onmouseover="ever_start()">
        <td style="padding:20px;width:25%;vertical-align:top">
          <div class="one">
            <div class="two" id='ever_image2'>
              <img src='images/eval.png' width=100%>
            </div>
            <img src='images/eval.png' width=100%>
          </div>
          <script type="text/javascript">
            function ever_start() {
              document.getElementById('ever_image2').style.opacity = "1";
            }
            function ever_stop() {
              document.getElementById('ever_image2').style.opacity = "0";
            }
            ever_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:top">
            <span class="papertitle">An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics</span>
          <br>
          <strong>Saba Ahmadi</strong>, 
          <a href="https://www.iro.umontreal.ca/~agrawal/">Aishwarya Agrawal</a>
          <br>
          <em>Findings of the Association for Computational Linguistics: EACL</em>, 2024
          <br>
          <a href="https://arxiv.org/abs/2305.14998">ArXiv</a>
        </td>
      </tr>

      <!-- Publication 3 -->
      <tr onmouseout="ever_stop()" onmouseover="ever_start()">
        <td style="padding:20px;width:25%;vertical-align:top">
          <div class="one">
            <div class="two" id='ever_image3'>
              <img src='images/mapl.png' width=100%>
            </div>
            <img src='images/mapl.png' width=100%>
          </div>
          <script type="text/javascript">
            function ever_start() {
              document.getElementById('ever_image3').style.opacity = "1";
            }
            function ever_stop() {
              document.getElementById('ever_image3').style.opacity = "0";
            }
            ever_stop()
          </script>
        </td>
        <td style="padding:20px;width:75%;vertical-align:top">
            <span class="papertitle">MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting</span>
          <br>
          <a href="https://oscmansan.github.io/">Oscar Mañas</a>, 
          <a href="https://es.linkedin.com/in/pau-rodriguez-lopez-5786a154">Pau Rodríguez*</a>, 
          <strong>Saba Ahmadi*</strong>, 
          <a href="http://www.aidanematzadeh.me/">Aida Nematzadeh</a>, 
          <a href="https://yash-goyal.github.io/">Yash Goyal</a>, 
          <a href="https://www.iro.umontreal.ca/~agrawal/">Aishwarya Agrawal</a>
          <br>
          * equal contribution
          <br>
          <em>The European Chapter of the Association for Computational Linguistics (EACL)</em>, 2023
          <br>
          <a href="https://arxiv.org/abs/2210.07179">ArXiv</a>
        </td>
      </tr>

    </table>

        </td>
      </tr>
    </table>
  </body>
</html>
